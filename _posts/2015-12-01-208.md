---
id: 208
title: Native Speed File Backed Large Data Storage In ‘Pure’ Java
date: 2015-12-01T01:00:31+00:00
author: Alexander Turner
layout: post
guid: http://www.javaadvent.com/?p=208
permalink: /2015/12/208.html
dsq_thread_id:
  - 4962579346
categories:
  - Uncategorized
---
<h3>Motivation:</h3>
All this started with the realisation that I could not afford a big enough computer. Audio processing requires huge amounts of memory. <a href="http://audacityteam.org/" target="_blank">Audacity</a>, an amazing free audio processor, manages this using a file backed storage system. This is a common approach for such issues where we store a huge amount of information and want random access to it. So, I wanted to develop a system for <a href="http://sonic-field.blogspot.co.uk/" target="_blank">Sonic Field</a> (my pet audio processing/synthesis project) which gave the same powerful disk based memory approach but in pure Java.

I got this to work late last year and discussed it (briefly) in the Java Advent Calendar (<a href="http://www.javaadvent.com/2014/12/a-serpentine-path-to-music.html" target="_blank">http://www.javaadvent.com/2014/12/a-serpentine-path-to-music.html</a>) overview of Sonic Field. Disk based memory allows Sonic Field to process audio systems which require huge amounts of memory on my humble 16 gigabyte laptop. For example, this recent piece took over 50 gigabytes of memory to create:

https://youtu.be/1_x7lNoQEA8

Whilst this was a breakthrough, it was also inefficient. Memory intensive operations like mixing were a bottle neck in this system. Here I turn Java into a memory power house by implementing the same system but very much more efficiently. I suspect I am getting near the limit at which Java is no longer at a performance disadvantage to C++.

Last year I gave a high level overview of the method; this year I am deep diving to implementation of performance details. In so doing I will explain how we can remove the overhead of traditional Java memory access techniques and then expand the ideas for a more general approach to sharing and persisting large memory systems in JVM programming.
<h3>What Is Segmented Storage?</h3>
<span style="font-weight: 400;">I admit there are a lot of concepts here. The first one to get our heads around is just how inefficient normal memory management of large memory systems is in Java. Let me be very clear indeed, I am not talking about garbage collection. Years of experience with both Java and C++ has taught me that neither collected nor explicit heap management is efficient or easy to get right. I am not discussing this at all. The issues with the JVM’s management of large memory systems are because of its bounds checking and object model. This is thrown into sharp focus when working with memory pools.</span>

<span style="font-weight: 400;">As latency or throughput performance become more critical than memory use there comes a point where one has to break out memory pools. Rather than a memory system which mixes everything together in one great glorious heap, we have pools of same sized objects. This requires more memory than a pure heap if the pool is not fully used or if the elements being mapped into pool chunks are smaller than the chunks themselves. However, pools are very fast indeed to manage.</span>

<span style="font-weight: 400;">In this post I am going to discuss pool backed segmented storage. Segmented storage is based on a pool but allows allocation of larger storage containers than a single pool chunk. The idea is that a storage container (say 1 gigabyte) can be made up of a selection of chunks (say 1 megabyte each). The segmented storage region is not necessarily made up of contiguous chunks. Indeed, this is its most important feature. It is made up of equal sized chunks from a backing pool but the chunks are scatters across virtual address space and might not even be in order. With this we have something with the request and release efficiency of a pool but but closer to the memory use efficiency of a heap and without any concerns over fragmentation.</span>

<span style="font-weight: 400;">Let us look at what a pool looks like first; then we can come back to segmentation.</span>

<span style="text-decoration: underline;"><span style="font-weight: 400;">A pool, in this discussion, consists of these parts:</span></span>
<ol>
	<li style="font-weight: 400;"><span style="font-weight: 400;">A pool (not necessarily all in one data structure) of chunks of equal sized memory.</span></li>
	<li style="font-weight: 400;"><span style="font-weight: 400;">One or more lists of used chunks.</span></li>
	<li style="font-weight: 400;"><span style="font-weight: 400;">One list of free chunks.</span></li>
</ol>
<span style="text-decoration: underline;"><span style="font-weight: 400;">To create a segmented memory allocation from a pool we have a loop:</span></span>
<ol>
	<li style="font-weight: 400;"><span style="font-weight: 400;">Create a container (array or some such) of memory chunks. Call this the segment list for the allocation.</span></li>
	<li style="font-weight: 400;"><span style="font-weight: 400;">Take a chunk of memory off the free list and add it to the segment list.</span></li>
	<li style="font-weight: 400;"><span style="font-weight: 400;">See if the segment list contains equal or more total memory than required.</span></li>
	<li style="font-weight: 400;"><span style="font-weight: 400;">If not repeat from 2.</span></li>
</ol>
<span style="font-weight: 400;">Now we have an allocation segment list which has at least enough memory for the requirement. When we free this memory we simply put the chunks back on the free list. We can see from this that very quickly the chunks on the free list will no longer be in order and even if we were to sort them by address, they still would not be contiguous. Thus any allocation will have enough memory but not in any contiguous order.</span>
<h4>Here is a worked example:</h4>
<span style="font-weight: 400;">We will consider 10 chunks of 1 megabyte which we can call 1,2...10 which are initial in order.</span>
<pre><span style="font-weight: 400;">Start:
</span><span style="line-height: 1.6471;">  Free List: 1 2 3 4 5 6 7 8 9 10</span></pre>
<pre><span style="font-weight: 400;">Allocate a 2.5 megabyte store:
</span><span style="line-height: 1.6471;">  Free List: 1 2 3 4 5 6 7
</span><span style="font-weight: 400;">  Allocated Store A: 8 9 10</span></pre>
<pre><span style="font-weight: 400;">Allocate a 6 megabyte store:
</span><span style="font-weight: 400;">  Free List: 1 
</span><span style="font-weight: 400;">  Allocated Store A: 8 9 10
</span><span style="font-weight: 400;">  Allocated Store A: 7 6 5 4 3 2</span></pre>
<pre><span style="font-weight: 400;">Free Allocated Store A:
</span><span style="line-height: 1.6471;">  Free List: 10 9 8 1
</span><span style="font-weight: 400;">  Allocated Store A: 7 6 5 4 3 2</span></pre>
<pre><span style="font-weight: 400;">Allocate a 3.1 megabyte store:
</span><span style="font-weight: 400;">  Free List: 
</span><span style="font-weight: 400;">  Allocated Store A: 7 6 5 4 3 2
</span><span style="font-weight: 400;">  Allocated Store C:10 9 8 1</span></pre>
<span style="font-weight: 400;">One can note that such an approach is good for some situations for systems like 64bit C++ but its true power is for Java. In current JVMs the maximum addressable array or ByteBuffer contains only 2**31 elements segmented storage offers an efficient way of addressing much greater quantities of memory and backing that memory with memory mapped files if required.. Consider that we need 20 billion doubles, we cannot allocate them into an array or a ByteBuffer; but we can use segmented memory so that we can achieve our goal.</span>

<span style="font-weight: 400;">Using anonymous virtual memory in Java for very large memory objects can be inefficient. In use cases where we want to handle very much more memory than the RAM on the machine, we are better off using memory mapped files than just using anonymous swap space. This means that the JVM is not competing with other programs for swap space (to an extent) but what is more important is that garbage collected memory distributes object access which is particularly poor for anonymous virtual memory. We want to concentrate access to particular pages in the time domain so that we attract as few hard page faults as possible. I have discuss other concepts in this area here: <a href="https://jaxenter.com/high-speed-multi-threaded-virtual-memory-in-java-105629.html" target="_blank">https://jaxenter.com/high-speed-multi-threaded-virtual-memory-in-java-105629.html</a>.</span>

<span style="font-weight: 400;">Given this. if we narrow our requirement to 20 billion doubles as a memory mapped file then we are not even going to be able to use magic in sun.misc.Unsafe (see later) to help. Without JNI the largest memory mapped file ‘chunk’ we can manage in Java is just 2^31 bytes. It is this requirement for memory mapped files and the inherent allocation/freeing efficiency of segmented storage approaches that lead to me using it for Sonic Field (where I often need to manage over 100G of memory on a 16G machine). </span>
<h4>Drilling Into The Implementation:</h4>
<span style="font-weight: 400;">We now have a clear set of ideas to implement. We need mapped byte buffers. Each buffer is a chunk in a pool for free chunks. When we want to allocate a storage container we need to take some of these mapped byte buffer chunks out of the free pool and into our container. When the container is freed we return our chunks to the free pool. Simple, efficient and clean.</span>

<span style="font-weight: 400;">Also, one important thing is that the mapped byte buffers are actually java.nio.DirectByteBuffer objects with file back memory. We will use this concept later; for now we can just think of them as ByteBuffers.</span>

<span style="font-weight: 400;">On Sonic Field (which is the code for which I developed the technique of segmented storage using mapped byte buffers. - see </span><a href="https://github.com/nerds-central/SonicFieldRepo"><span style="font-weight: 400;">https://github.com/nerds-central/SonicFieldRepo</span></a><span style="font-weight: 400;">). In that code base I have defined the following:</span>
<pre><span style="font-weight: 400;">    private static final long  CHUNK_LEN        = 1024 * 1024;</span></pre>
<span style="font-weight: 400;">To get the sample we can consider each chunk as a CHUNK_LEN ByteBuffer. The code for accessing an element from an allocated memory chunk was before my speedup work:</span>
<pre><span style="font-weight: 400;">   private static final long  CHUNK_SHIFT      = 20;
</span><span style="font-weight: 400;">   private static final long  CHUNK_MASK       = CHUNK_LEN - 1;
</span><span style="font-weight: 400;">...
</span><span style="line-height: 1.6471;">   public final double getSample(int index)
</span><span style="line-height: 1.6471;">   {
</span><span style="line-height: 1.6471;">       long bytePos = index &lt;&lt; 3;
</span><span style="line-height: 1.6471;">       long pos = bytePos &amp; CHUNK_MASK;
</span><span style="line-height: 1.6471;">       long bufPos = (bytePos - pos) &gt;&gt; CHUNK_SHIFT;
</span><span style="line-height: 1.6471;">       return chunks[(int) bufPos].getDouble((int) pos);
</span><span style="line-height: 1.6471;">   }</span></pre>
<span style="text-decoration: underline;"><span style="font-weight: 400;">So the allocated segment list in this case is an array of ByteBuffers:</span></span>
<ol>
	<li style="font-weight: 400;"><span style="font-weight: 400;">Find the index into the list by dividing the index required by the chunk size (use shift for efficiency).</span></li>
	<li style="font-weight: 400;"><span style="font-weight: 400;">Find the index into the found chunk by taking the modulus (use binary <em>and</em> for efficiency).</span></li>
	<li style="font-weight: 400;"><span style="font-weight: 400;">Look up the actual value using the getDouble intrinsic method (looks like a method but the compiler knows about it an elides the method call).</span></li>
</ol>
<span style="font-weight: 400;">All this looks fine, but it does not work out all that well because there are some fundamental issues with the way Java lays out objects in memory which prevent segmented access being properly optimised. On the face of it, accessing a segmented memory area should be a few very fast shift and logic operations and an indirect lookup but that does not work out so for Java; all the problems happen in this line:</span>
<pre><span style="font-weight: 400;"> return chunks[(int) bufPos].getDouble((int) pos);</span></pre>
<span style="font-weight: 400;">This is what this line has to do:</span>
<ol>
	<li style="font-weight: 400;"><span style="font-weight: 400;">Look up the chunks object from its handle.</span></li>
	<li style="font-weight: 400;"><span style="font-weight: 400;">Bounds check.</span></li>
	<li style="font-weight: 400;"><span style="font-weight: 400;">Get the data from its data area.</span></li>
	<li style="font-weight: 400;"><span style="font-weight: 400;">From that object handle for the ByteBuffer look up actual object.</span></li>
	<li style="font-weight: 400;"><span style="font-weight: 400;">Look up its length dynamically (it can change so this is a safe point and an object field lookup).</span></li>
	<li style="font-weight: 400;"><span style="font-weight: 400;">Bounds check.</span></li>
	<li style="font-weight: 400;"><span style="font-weight: 400;">Retrieve the data.</span></li>
</ol>
<span style="font-weight: 400;"><em>Really?</em> Yes, the JVM does all of that which is quite painful. Not only is it a lot of instructions it also requires jumping around in memory will all the consequent cache line flushing and memory pauses.</span>

<span style="font-weight: 400;">How can we improve on this? Remember that our ByteBuffers are DirectByteBuffers, this means that their data is not stored on the Java heap; it is located in the same virtual address location throughout the object lifetime. I bet you have guessed that the key here is using sun.misc.Unsafe. Yes, it is; we can bypass all this object lookup by using offheap memory. To do so means bending a few Java and JVM rules but the dividends are worth it.</span>
<p style="text-align: center;"><b>From now on everything I discuss is relevant to Java 1.8 x86_64. Future versions might break this approach as it is not standards compliant.</b></p>
<span style="text-decoration: underline;"><span style="font-weight: 400;">Consider this:</span></span>
<pre><span style="font-weight: 400;">   private static class ByteBufferWrapper
</span><span style="font-weight: 400;">   {
</span><span style="line-height: 1.6471;">       public long       address;
</span><span style="line-height: 1.6471;">       public ByteBuffer buffer;
</span><span style="line-height: 1.6471;">       public ByteBufferWrapper(ByteBuffer b) throws
</span><span style="font-weight: 400;">                      NoSuchMethodException,
</span><span style="font-weight: 400;">                      SecurityException,
</span><span style="font-weight: 400;">                      IllegalAccessException,
                      IllegalArgumentException,
</span><span style="font-weight: 400;">                      InvocationTargetException
</span><span style="line-height: 1.6471;">       {
</span><span style="font-weight: 400;">           Method addM = b.getClass().getMethod("address");
</span><span style="font-weight: 400;">           addM.setAccessible(true);
</span><span style="line-height: 1.6471;">           address = (long) addM.invoke(b);
</span><span style="line-height: 1.6471;">           buffer = b;
</span><span style="line-height: 1.6471;">       }
</span><span style="line-height: 1.6471;">   }</span></pre>
<span style="font-weight: 400;">What we are doing is getting the address in memory of the data stored in a DirectByteBuffer. To do this I use reflection as DirectByteBuffer is package private. DirectByteBuffer has a method on it called address() which returns a long. On x86_64 the size of an address (64 bits) is the same as long. Whilst the value of long is signed, we can just used long as binary data and ignore its numerical value. So the long returned from address() is actually the virtual address of the start of the buffer’s storage area.</span>

<span style="font-weight: 400;">Unlike ‘normal’ JVM storage (e.g. arrays) the storage of a DirectByteBuffer is ‘off heap’. It is virtual memory just like any other, but it is not owned by the garbage collector and cannot be moved by the garbage collector; this makes a huge difference to how quickly and by which techniques we can access it. Remember, the address returned by address() never changes for a given DirectByteBuffer object; consequently, we can use this address ‘forever’ and avoid object lookups.</span>
<h3>Introducing sun.misc.Unsafe:</h3>
<span style="font-weight: 400;">Whilst it would be lovely to believe that calling getDouble(int) on a DirectByteBuffer is super efficient, it does not appear that it is so. The bounds check slows it down despite the method being intrinsic [a magic function which the JVM JIT compiler knows about and can replace with machine code rather than compiling in a normal fashion]. However, with our address we can now use </span><a href="https://dzone.com/articles/understanding-sunmiscunsafe"><span style="font-weight: 400;">sun.misc.Unsafe</span></a><span style="font-weight: 400;"> to access the storage.</span>

<span style="font-weight: 400;">Rather than:</span>
<pre><span style="font-weight: 400;">b.getDouble(pos);</span></pre>
<span style="font-weight: 400;">We can:</span>
<pre><span style="font-weight: 400;">unsafe.getDouble(address+pos);</span></pre>
<span style="font-weight: 400;">The unsafe version is also intrinsic and compiles down to pretty much the same machine code as a C compiler (like gcc) would produce. In other words, it is as fast as it can get; there are no object dereferences or bounds checks, it just loads a double from an address. </span>

The store equivalent is:
<pre>unsafe.putDouble(address+pos,value);</pre>
What is this ‘unsafe’ thing? We get that with another reflection hack around:
<pre><span style="font-weight: 400;">   private static Unsafe getUnsafe()
</span><span style="line-height: 1.6471;">   {
</span><span style="line-height: 1.6471;">       try
</span><span style="line-height: 1.6471;">       {
</span><span style="line-height: 1.6471;">           Field f = Unsafe.class.getDeclaredField("theUnsafe");
</span><span style="line-height: 1.6471;">           f.setAccessible(true);
</span><span style="line-height: 1.6471;">           return (Unsafe) f.get(null);
</span><span style="line-height: 1.6471;">       }
</span><span style="line-height: 1.6471;">       catch (Exception e)
</span><span style="line-height: 1.6471;">       {
</span><span style="line-height: 1.6471;">           throw new RuntimeException(e);
</span><span style="line-height: 1.6471;">       }
</span><span style="line-height: 1.6471;">   }
</span><span style="line-height: 1.6471;">   private static final Unsafe unsafe = getUnsafe();</span></pre>
<span style="font-weight: 400;">It is important to load the unsafe singleton into a final static field. This allows the compiler to assume the object reference never changes and so the very most optimal code is generated.</span>

<span style="font-weight: 400;">Now we have very fast acquisition of data from a DirectByteBuffer but we have a segmented storage model so we need to get the address for the correct byte buffer very quickly. If we store these in an array we risk the array bounds check and the array object dereference steps. We can get rid of these by further use of unsafe and offheap memory.</span>
<pre><span style="font-weight: 400;">   private final long  chunkIndex;
</span><span style="font-weight: 400;">...
</span><span style="line-height: 1.6471;">   try
</span><span style="line-height: 1.6471;">   {
</span><span style="line-height: 1.6471;">       // Allocate the memory for the index - final so do it here
</span><span style="line-height: 1.6471;">       long size = (1 + ((l &lt;&lt; 3) &gt;&gt; CHUNK_SHIFT)) &lt;&lt; 3;
</span><span style="line-height: 1.6471;">       allocked = chunkIndex = unsafe.allocateMemory(size);
</span><span style="line-height: 1.6471;">       if (allocked == 0)
</span><span style="line-height: 1.6471;">       {
</span><span style="line-height: 1.6471;">           throw new RuntimeException("Out of memory allocating " + size);
</span><span style="line-height: 1.6471;">      }
</span><span style="line-height: 1.6471;">      makeMap(l &lt;&lt; 3l);
</span><span style="line-height: 1.6471;">   }
</span><span style="line-height: 1.6471;">   catch (Exception e)
</span><span style="line-height: 1.6471;">   {
</span><span style="line-height: 1.6471;">       throw new RuntimeException(e);
</span><span style="line-height: 1.6471;">   }</span></pre>
<span style="font-weight: 400;">Again we use the ‘final’ trick to let the compiler make the very best optimisations. The final here is a long which is just an address. We can directly allocate offheap memory using unsafe. The imaginatively called function to do this is allocateMemory(long). This returns a long which we store in chunkIndex. allocateMemory(long) actually allocates bytes but we want to store what is effectively an array of longs (addresses); this is what the bit of bit twiddling logic is doing when it computes size.</span>

<span style="font-weight: 400;">Now that we have a chunk of offheap memory large enough to store the addresses for the DirectByteBuffer segments for our storage container we can put the addresses in and retrieve them using unsafe.</span>

<strong>During storage construction we:</strong>
<pre><span style="font-weight: 400;">    // now we have the chunks we get the address of the underlying memory
</span><span style="line-height: 1.6471;">   // of each and place that in the off heap lookup so we no longer
</span><span style="line-height: 1.6471;">   // reference them via objects but purely as raw memory
</span><span style="line-height: 1.6471;">   long offSet = 0;
</span><span style="line-height: 1.6471;">   for (ByteBufferWrapper chunk : chunks)
</span><span style="line-height: 1.6471;">   {
</span><span style="line-height: 1.6471;">       unsafe.putAddress(chunkIndex + offSet, chunk.address);
</span><span style="line-height: 1.6471;">       offSet += 8;
</span><span style="line-height: 1.6471;">   }</span></pre>
<span style="font-weight: 400;">Which means our new code for getting and setting data can be very simple indeed:</span>
<pre><span style="font-weight: 400;">    private long getAddress(long index)
</span><span style="line-height: 1.6471;">   {
</span><span style="line-height: 1.6471;">       long bytePos = index &lt;&lt; 3;
</span><span style="line-height: 1.6471;">       long pos = bytePos &amp; CHUNK_MASK;
</span><span style="line-height: 1.6471;">       long bufPos = (bytePos - pos) &gt;&gt; CHUNK_SHIFT;
</span><span style="line-height: 1.6471;">       long address = chunkIndex + (bufPos &lt;&lt; 3);
</span><span style="line-height: 1.6471;">       return unsafe.getAddress(address) + pos;
</span><span style="line-height: 1.6471;">   }
</span><span style="line-height: 1.6471;">
   /* (non-Javadoc)
</span><span style="line-height: 1.6471;">    * @see com.nerdscentral.audio.SFSignal#getSample(int)
</span><span style="line-height: 1.6471;">    */
</span><span style="line-height: 1.6471;">   @Override
</span><span style="line-height: 1.6471;">   public final double getSample(int index)
</span><span style="line-height: 1.6471;">   {
</span><span style="line-height: 1.6471;">       return unsafe.getDouble(getAddress(index));
</span><span style="line-height: 1.6471;">   }
</span><span style="line-height: 1.6471;">
   /* (non-Javadoc)
</span><span style="line-height: 1.6471;">    * @see com.nerdscentral.audio.SFSignal#setSample(int, double)
</span><span style="line-height: 1.6471;">    */
</span><span style="line-height: 1.6471;">   @Override
</span><span style="line-height: 1.6471;">   public final double setSample(int index, double value)
</span><span style="line-height: 1.6471;">   {
</span><span style="line-height: 1.6471;">       unsafe.putDouble(getAddress(index), value);
</span><span style="line-height: 1.6471;">       return value;
</span><span style="line-height: 1.6471;">   }</span></pre>
<span style="font-weight: 400;">The wonderful thing about this is the complete lack of object manipulation or bounds checking. OK, if someone asks for at sample which is out of bounds, the JVM will crash. That might not be a good thing. This sort of programming is very alien to many Java coders and we need to take its dangers very seriously. However, it is really quite fast compared to the original.</span>

In my experiments, I have found that the default JVM inline settings are a little too conservative to get the best out of this approach. I have seen large speedups (up to a two times performance improvement) with the following command line tweaks.
<pre>-XX:MaxInlineSize=128 -XX:InlineSmallCode=1024</pre>
These just let the JVM make a better job of utilising the extra performance available through not being forced to perform bounds checks and object lookups. In general, I would not advise fiddling with JVM inline settings, but in this case I have real benchmark experience to show a benefit for complex offheap access work.
<h3>Testing - How Much Faster Is It?</h3>
<strong>I wrote the following piece of Jython to test:</strong>
<pre><span style="font-weight: 400;">import math
</span><span style="font-weight: 400;">from java.lang import System

</span><span style="font-weight: 400;">sf.SetSampleRate(192000)
</span><span style="font-weight: 400;">count=1000
</span><span style="font-weight: 400;">ncount=100
</span><span style="font-weight: 400;">
def test():
</span><span style="line-height: 1.6471;">   t1=System.nanoTime()
</span><span style="line-height: 1.6471;">   for i in range(1,ncount):
</span><span style="line-height: 1.6471;">       signal=sf.Mix(+signal1,+signal2)
</span><span style="line-height: 1.6471;">       signal=sf.Realise(signal)
</span><span style="line-height: 1.6471;">       -signal
</span><span style="line-height: 1.6471;">   t2=System.nanoTime()
</span><span style="line-height: 1.6471;">   d=(t2-t1)/1000000.0
</span><span style="line-height: 1.6471;">   print "Done: " + str(d)
</span><span style="line-height: 1.6471;">   return d
</span><span style="font-weight: 400;">
signal1=sf.Realise(sf.WhiteNoise(count))
</span><span style="font-weight: 400;">signal2=sf.Realise(sf.WhiteNoise(count))
</span><span style="font-weight: 400;">print "WARM"
</span><span style="font-weight: 400;">for i in range(1,100):
</span><span style="line-height: 1.6471;">   test()
</span><span style="line-height: 1.6471;">   
</span><span style="font-weight: 400;">print "Real"
</span><span style="font-weight: 400;">total=0.0
</span><span style="font-weight: 400;">for i in range(1,10):
</span><span style="line-height: 1.6471;">   total+=test()
</span><span style="font-weight: 400;">
print "Mean " + str(total/9.0)
</span><span style="font-weight: 400;">
-signal1
</span><span style="font-weight: 400;">-signal2</span></pre>
<span style="font-weight: 400;">What this does is create some stored doubles and then create new ones and reading from the old into the new over and over. Remember that we are using segmented storage backed by a pool; consequently, we only truly allocate that storage initially and after that the ‘chunks’ are just recycled. This architecture means that our execution time is dominated by executing getSample and setSample, not allocation or any other paraphernalia. </span>

How much faster is our off heap system? On my Macbook Pro Retina I7 machine with Java 1.8.0 I got these figures for the ‘Real’ (i.e. post warm up) operations (smaller is better):

<strong>For the unsafe memory model:</strong>
<ul>
	<li><span style="font-weight: 400;">Done: 187.124</span></li>
	<li><span style="font-weight: 400;">Done: 175.007</span></li>
	<li><span style="font-weight: 400;">Done: 181.124</span></li>
	<li><span style="font-weight: 400;">Done: 175.384</span></li>
	<li><span style="font-weight: 400;">Done: 180.497</span></li>
	<li><span style="font-weight: 400;">Done: 180.688</span></li>
	<li><span style="font-weight: 400;">Done: 183.309</span></li>
	<li><span style="font-weight: 400;">Done: 178.901</span></li>
	<li><span style="font-weight: 400;">Done: 181.746</span></li>
	<li><span style="font-weight: 400;">Mean 180.42</span></li>
</ul>
<strong>For the traditional memory model:</strong>
<ul>
	<li><span style="font-weight: 400;">Done: 303.008</span></li>
	<li><span style="font-weight: 400;">Done: 328.763</span></li>
	<li><span style="font-weight: 400;">Done: 299.701</span></li>
	<li><span style="font-weight: 400;">Done: 315.083</span></li>
	<li><span style="font-weight: 400;">Done: 306.809</span></li>
	<li><span style="font-weight: 400;">Done: 302.515</span></li>
	<li><span style="font-weight: 400;">Done: 304.606</span></li>
	<li><span style="font-weight: 400;">Done: 300.291</span></li>
	<li><span style="font-weight: 400;">Done: 342.436</span></li>
	<li><span style="font-weight: 400;">Mean 311.468</span></li>
</ul>
<span style="font-weight: 400;">So our unsafe memory model is </span><b><i>1.73 times</i></b><span style="font-weight: 400;"> faster than the traditional Java approach!</span>
<h3>Why Is It 1.73 Times Faster</h3>
<span style="font-weight: 400;">We can see why. </span>

<strong>If we look back at the list of things required to just read a double from the traditional DirectByteBuffer and array approach:</strong>
<ol>
	<li style="font-weight: 400;"><span style="font-weight: 400;">Look up the chunks object from its handle.</span></li>
	<li style="font-weight: 400;"><span style="font-weight: 400;">Bounds check.</span></li>
	<li style="font-weight: 400;"><span style="font-weight: 400;">Get the data from its data area.</span></li>
	<li style="font-weight: 400;"><span style="font-weight: 400;">From that object handle for the ByteBuffer look up actual object.</span></li>
	<li style="font-weight: 400;"><span style="font-weight: 400;">Look up its length dynamically (it can change so this is a safe point and an object field lookup).</span></li>
	<li style="font-weight: 400;"><span style="font-weight: 400;">Bounds check.</span></li>
	<li style="font-weight: 400;"><span style="font-weight: 400;">Retrieve the data.</span></li>
</ol>
<strong>With the new approach we have:</strong>
<ol>
	<li style="font-weight: 400;"><span style="font-weight: 400;">Retrieve the address of the chunk</span></li>
	<li style="font-weight: 400;"><span style="font-weight: 400;">Retrieve the data from that chunk</span></li>
</ol>
<span style="font-weight: 400;">Not only are there very many fewer machine instructions being issued, the memory access is much more localised which almost certainly enhances the cache usage during data processing.</span>

<span style="font-weight: 400;">The source code for the fast version of the storage system as described here is: </span><a href="https://github.com/nerds-central/SonicFieldRepo/blob/cf6a1b67fb8dd07126b0b1274978bd850ba76931/SonicField/src/com/nerdscentral/audio/SFData.java"><span style="font-weight: 400;">https://github.com/nerds-central/SonicFieldRepo/blob/cf6a1b67fb8dd07126b0b1274978bd850ba76931/SonicField/src/com/nerdscentral/audio/SFData.java</span></a>

I am hoping that you, the reader, have spotted one big problem I have not addressed! My code is allocating offheap memory when ever it creates a segmented storage container. However, this memory will not be freed by the garbage collector. We could try to free with with finalizers but there are many reasons why this is not such a great idea.

<span style="font-weight: 400;">My solution is to use explicit resource management. Sonic Field uses try with resources to manage its memory via reference counts. When the reference count for a particular storage container hits zero, the container is freed which places it storage chunks back in the free list and uses unsafe to free the address lookup memory.</span>
<h2>Other Uses And New Ideas</h2>
Nearly a year ago now I posted '<a href="http://nerds-central.blogspot.co.uk/2015/01/java-power-features-to-stay-relvant.html" target="_blank">Java Power Features To Stay Relevant</a>'; I guess it was a controversial post and not everyone I have spoken to about my ideas finds them agreeable (to say the least). Nevertheless, I still believe the JVM has a challenge on its hands. The complex multi-threaded model of Java and the JVM its self is not necessarily the huge benefit people think it should be in the world of multi-core computing. There is still a lot of interest in using multiple small processes which communicate via shared memory or sockets. With the slow but inevitable increase in RDMA based networking, these approaches will seem more and more natural to people.

Java and JVM languages seem to have managed to make themselves uniquely unable to take advantage of these shifts in thinking. By developing a 'walled garden' approach the JVM has become very efficient at working internally but not great at working with other processes. This is a performance issue and also a stability issue; no matter how hard we try, there is always a chance the JVM will crash or enter an unstable state (OutOfMemoryError anyone?). In production systems this often necessitates several small JVM instances working together so if one goes away the production system stays up. Memory mapped files are a great way to help with persisting data even when a JVM process goes away.

All these issues lead me to another reason I am very interested in efficient offheap, mapped file architectures for the JVM. This technology sits at the overlap of shared memory and mapped file technologies which are now driving forces behind high speed, stable production environments. Whilst the system I discussed here is for a single JVM, using offheap atomics (see here: <a href="http://nerds-central.blogspot.co.uk/2015/05/synchronising-sunmiscunsafe-with-c.html" target="_blank">http://nerds-central.blogspot.co.uk/2015/05/synchronising-sunmiscunsafe-with-c.html</a>) we can put the  free list offheap and share it between processes. Shared memory queues can then also give interprocess arbitration of segmented storage allocation and utilisation. Suddenly, the segmented storage model becomes an efficient way for multiple processes, both JVM and other technologies (Python, C++ etc) to share large, file persisted memory systems.

Right now there are some issues. The biggest of which is that whilst Java supports shared memory via memory mapped files it does not support that via pure shared memory. File mapping is an advantage if we are interested in large areas of memory (as in this example) but it is an unnecessary performance issue for small areas of rapidly changing memory which do not require persistence. I would like to see a true shared memory library in the JDK; this is unlikely to happen any time soon (see my point about a walled garden). JNI offers a route but then JNI has many disadvantages we well. Maybe project Panama will give the required functionality and finally break down the JVM's walls.

To bring all this together the next trick I want to try is mapping files to a ramdisk (there is an interesting write up on this here: <a href="http://www.jamescoyle.net/knowledge/951-the-difference-between-a-tmpfs-and-ramfs-ram-disk)" target="_blank">http://www.jamescoyle.net/knowledge/951-the-difference-between-a-tmpfs-and-ramfs-ram-disk)</a>. This should be quite easy on Linux and would let us place interprocess queues in a pure RAM shared memory areas without using JNI. With this piece done, a pure Java high speed interprocess shared memory model would be insight. Maybe that will have to wait for next year's calendar?